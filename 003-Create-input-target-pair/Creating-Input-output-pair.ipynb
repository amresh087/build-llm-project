{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3349abc3-117d-454a-bc2a-760692586a03",
   "metadata": {},
   "source": [
    "# Creating-Input-output-pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a27db68-8e70-4c52-9d26-4bc078f03a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of charecter 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "with open(\"the-verdict.txt\",\"r\", encoding=\"utf-8\")as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"total number of charecter\", len(raw_text))\n",
    "    \n",
    "print(raw_text[:100])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0949601d-b9f3-44b8-aa37-9e64b1a2f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "enc_text=tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d88c2-7dc0-43d7-b405-72af993381a3",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4CAF50;padding: 10px;background-color: #E8F5E9;color: #333;font-weight: bold;\">\n",
    "creating another variable. Now we are removing first 50 token from exiting token\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1bd0b37-b403-4c5e-b652-386416eb4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample=enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4152d3-3b83-4a69-be16-b522161c1f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:[290, 4920, 2241, 287]\n",
      "y:[4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "# now we need to define context size\n",
    "context_size=4\n",
    "# the context size 4 means the model is trained to look at a seaquence of 4 token to predict the next word in sequence\n",
    "# the input x is first 4 token [1,2,3,4] and the target y is first 4 token is [2,3,4,5]\n",
    "# mean if you give 1 as input then output will be 2 and if input 1,2 then output 3\n",
    "x=enc_sample[:context_size]\n",
    "y=enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x:{x}\")\n",
    "print(f\"y:{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a43e775-bf32-44a5-99e8-46ebd1a7b61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ======== 4920\n",
      "[290, 4920] ======== 2241\n",
      "[290, 4920, 2241] ======== 287\n",
      "[290, 4920, 2241, 287] ======== 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context=enc_sample[:i]\n",
    "    desired=enc_sample[i]\n",
    "\n",
    "    print(context,\"========\",desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491f081-13a1-4ce7-ac75-603ee752555d",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4CAF50;padding: 10px;background-color: #E8F5E9;color: #333;font-weight: bold;\">\n",
    "Decode the text from the token\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef830b7-0a0a-46b6-b1ef-02e95eeeea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ========  established\n",
      " and established ========  himself\n",
      " and established himself ========  in\n",
      " and established himself in ========  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"========\", tokenizer.decode([desired]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9060db-b54a-4698-9fa4-cffa743ddae7",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #FB8C00; padding: 10px; background-color: #FFF3E0; color: #333; font-weight: bold;\">\n",
    "    Now we need to create it much more structure manor. Becuase we need to do pallel processing. Suppose we have multiple cpu then we need to do pallel processing. Hence we need to doing computing in batches \n",
    "\n",
    "For this we need to use data loader\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc861ffa-4e9c-4702-b8b5-e22d57207dd5",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #FB8C00; padding: 10px; background-color: #FFF3E0; color: #333; font-weight: bold;\">\n",
    "    While using a DataLoader, it iterates over the input dataset and returns\n",
    "    inputs and targets as PyTorch tensors, typically in the form of\n",
    "    multi-dimensional arrays.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004dd75-1f8e-47d6-89cc-7d5f30e5d106",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4CAF50;padding: 10px;background-color: #E8F5E9;color: #333;font-weight: bold;\">\n",
    "     Implemeting DataLoader (DataSet and Data Loader)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e008a-023f-428a-8293-7caf7f39337f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
