{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaaada98-dd38-4b04-a4cb-b7d329aa5a4d",
   "metadata": {},
   "source": [
    "# Step 1 : Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff2fd4be-8618-427e-9d21-6282699f5fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'this', ',', '', ' ', 'is', ' ', 'the', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text=\"Hello, world. this, is the test.\"\n",
    "result=re.split(r'([,.]|\\s)',text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbb5aad-a2a1-46a5-a3e8-5daaa11ccdfa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "now we need to remove whitespace\n",
    "    \n",
    "but that is not mandatory suppose you are training python code or yml then whitespace having meaning \n",
    "item for item in result Iterates through each element of the list result.\n",
    "if item.split() item.split() splits the string by whitespace.\n",
    "If the string is empty or contains only spaces, item.split() returns an empty list:\n",
    " \"\" .split() → []\n",
    " \" \".split() → []\n",
    " An empty list is treated as False in Python. So, the condition if item.split() becomes:\n",
    " True → keep the item\n",
    " False → discard the item\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "582c6c58-fc1d-439b-a472-cc79aa8d072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'this', ',', 'is', 'the', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result=[item for item in result if item.split()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d0b808-6893-4773-99f7-c56c664fa47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd82f07f-91ab-42da-b305-c03d9a3eaf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'this', ',', '', ' ', 'is', '--', '', ' ', 'the', ' ', 'test', '?', '']\n",
      "['Hello', ',', 'world', '.', 'this', ',', 'is', '--', 'the', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text=\"Hello, world. this, is-- the test?\"\n",
    "result=re.split(r'([,.:;?!\"]|--|\\s)',text)\n",
    "print(result)\n",
    "result=[item for item in result if item.split()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e58bed4-5f84-445a-8076-26b6abe26f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of charecter 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "#in python open and read method present\n",
    "\n",
    "with open(\"the-verdict.txt\",\"r\", encoding=\"utf-8\")as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"total number of charecter\", len(raw_text))\n",
    "    \n",
    "print(raw_text[:100])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a6471ab9-9fd1-4d1a-89fe-acaddd6f0861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " token size : 4483\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "preprocess=re.split(r'([,.:;?!)(\"]|--|\\s)',raw_text)\n",
    "preprocess=[item for item in preprocess if item.split()]\n",
    "token_size=len(preprocess)\n",
    "print(\" token size :\", token_size)\n",
    "print(preprocess[:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c5387-1d87-4ae0-a0e5-29ba72fe033d",
   "metadata": {},
   "source": [
    "# Step 2 : creating token id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10a32c0-46cc-4c30-96ae-772de0cac0b7",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "while we are genrating token id then first we need to create vocabulary. While creating vocabulary then all token will be in sorted in alphabetical order\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cc2a5f22-c265-479a-85b0-488d40834c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocablary size : 1163\n"
     ]
    }
   ],
   "source": [
    "all_world=sorted(set(preprocess))\n",
    "vocablary_size=len(all_world)\n",
    "print(\"vocablary size :\",vocablary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "180185a7-2531-4469-a826-973f4c43082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! 0\n",
      "\" 1\n",
      "' 2\n",
      "'Are 3\n",
      "'It's 4\n",
      "'coming' 5\n",
      "'done' 6\n",
      "'subject 7\n",
      "'technique' 8\n",
      "'way 9\n",
      "( 10\n",
      ") 11\n",
      ", 12\n",
      "-- 13\n",
      ". 14\n",
      ": 15\n",
      "; 16\n",
      "? 17\n",
      "A 18\n",
      "Ah 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab={token:integer for integer,token in enumerate(all_world)}\n",
    "\n",
    "#enumerate(all_world)  --->> gives (index, word)\n",
    "#token: intrger.  --->> word : number\n",
    "#vocab = {...} --->>creates dictionary\n",
    "\n",
    "for i, (token, tokenid) in enumerate(vocab.items()):\n",
    "    if i == 20:\n",
    "        break\n",
    "    print(token, tokenid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac357a4c-65fd-4688-8f30-1a24041afc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now here we can think we have token and token id mean encoder we are passing text then genrate token id\n",
    "#but in decoder side we will pass tokenid then we need to get token text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d415a4-76f5-444c-b404-590943e388a1",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Now we are going to create a class called Tokenizer in python. In calss having encoder and decoder method\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   Encoder method:  we are taking simple text then genrate token and assign tokenID\n",
    "</div>\n",
    "\n",
    "\n",
    "**Decoder method: it is reverse to encoder we are taking tokenId then convert it token after that simple text**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c29c0935-74d4-4504-9f6b-f27b828349e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str={tokenid:token for token,tokenid in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocess=re.split(r'([,.:;?!\"]|--|\\s)',text)\n",
    "        preprocess=[item for item in preprocess if item.split()]\n",
    "        ids=[self.str_to_int[s] for s in preprocess]\n",
    "        return ids\n",
    "\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Join tokens back to text\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids)\n",
    "        # Remove space before punctuation\n",
    "        text = re.sub(r'\\s+([,.:;?!\\\"()\\]])', r'\\1', text)\n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dc79f8f6-f9a6-47c7-af45-999992418f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 69, 1016, 634, 561, 778, 12, 1157, 628, 12, 1, 81, 14, 44, 880, 1139, 786, 825, 14]\n"
     ]
    }
   ],
   "source": [
    "tokenizer=SimpleTokenizerV1(vocab)\n",
    "#Now we can test our encoder by passing some text from story which is loaded\n",
    "\n",
    "text=\"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids=tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e3826843-edd5-4df5-9ecf-6581ada2c4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\'s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d3095-eea7-48a5-af0c-3943715a8873",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  if you pass some text which is not present in vocablary then it will give error that is is resion LLM is trained large amount of the data and real LLM like chat GPT it is using billion of data ans also added special context token for handling this king of error \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ae5fb88f-e8b3-45e4-a7a8-6a9e6a2773cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text=\"\"\"\"Hello let us take tea\"\"\"\n",
    "#ids=tokenizer.encode(text)\n",
    "#print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f4d33e-5515-4f3d-9df1-5d8a1981af4d",
   "metadata": {},
   "source": [
    "# Special context token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d6fbfda3-f761-452c-ab23-5867b9056135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocablary size : 1165\n"
     ]
    }
   ],
   "source": [
    "all_token=sorted(set(preprocess))\n",
    "all_token.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "vocablary_size=len(all_token)\n",
    "print(\"vocablary size :\",vocablary_size)\n",
    "vocab1={token:integer for integer,token in enumerate(all_token)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "73b35dad-0787-4a74-87cd-d53a9ad4b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str={tokenid:token for token,tokenid in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocess=re.split(r'([,.:;?!\"]|--|\\s)',text)\n",
    "        preprocess=[item for item in preprocess if item.split()]\n",
    "        preprocess=[item if item in self.str_to_int\n",
    "                   else \"<|unk|>\" for item in preprocess\n",
    "                   ]\n",
    "        \n",
    "        ids=[self.str_to_int[s] for s in preprocess]\n",
    "        return ids\n",
    "\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Join tokens back to text\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids)\n",
    "        # Remove space before punctuation\n",
    "        text = re.sub(r'\\s+([,.:;?!\\\"()\\]])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0a67f132-89dd-4b93-a61b-99472e961c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1164, 653, 1086, 1001, 1004]\n"
     ]
    }
   ],
   "source": [
    "tokenizer1=SimpleTokenizerV2(vocab1)\n",
    "text1=\"\"\"\"Hello let us take tea\"\"\"\n",
    "ids1=tokenizer1.encode(text1)\n",
    "print(ids1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cc1abd46-c1ae-4ea8-89df-956d4fbfdf2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" <|unk|> let us take tea'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1.decode(ids1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc736242-01f2-4266-afc2-49ba2510eefe",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  Let test <|endoftext|>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d7551ee7-2858-4457-92d1-f293789dab0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hello let us take tea <|endoftext|> And his tone told me in a flash that he never thought of anything else.\n"
     ]
    }
   ],
   "source": [
    "text8=\"\"\"\"Hello let us take tea\"\"\"\n",
    "text10=\"\"\"And his tone told me in a flash that he never thought of anything else.\"\"\"\n",
    "newtext=\" <|endoftext|> \".join((text8,text10))\n",
    "print(newtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "89055ef2-cc3a-4f17-9cd0-69d5ac64571f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1164,\n",
       " 653,\n",
       " 1086,\n",
       " 1001,\n",
       " 1004,\n",
       " 1163,\n",
       " 21,\n",
       " 579,\n",
       " 1048,\n",
       " 1047,\n",
       " 695,\n",
       " 600,\n",
       " 147,\n",
       " 475,\n",
       " 1015,\n",
       " 561,\n",
       " 739,\n",
       " 1033,\n",
       " 754,\n",
       " 193,\n",
       " 415,\n",
       " 14]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1.encode(newtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b7d89ff3-9010-4e55-beb9-b2ee4d3e0f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1164, 653, 1086, 1001, 1004, 1163, 21, 579, 1048, 1047, 695, 600, 147, 475, 1015, 561, 739, 1033, 754, 193, 415, 14]\n"
     ]
    }
   ],
   "source": [
    "newids=tokenizer1.encode(newtext)\n",
    "print(newids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "05fb7423-35ed-4735-a68c-16bb4de97d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" <|unk|> let us take tea <|endoftext|> And his tone told me in a flash that he never thought of anything else.'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1.decode(newids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
